{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense, LSTM\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "def flatten(l):\n",
    "    try:\n",
    "        return flatten(l[0]) + (flatten(l[1:]) if len(l) > 1 else []) if type(l) is list else [l]\n",
    "    except IndexError:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare variables / parameters\n",
    "\n",
    "FILEPATH = './HDeviceCGM.csv'\n",
    "FIXED_WINDOW_SIZE = 10\n",
    "TRAIN_PCT = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding the data\n",
    "In this dataset we will be provided with dataset containing the following columns of importance\n",
    "1. RecID - unique record ID\n",
    "2. DeviceDtTmDaysFromEroll (Date time index)\n",
    "3. GlucoseValue (mg/dL) - the output glucose number\n",
    "Prepare the dataset, filter out the necessary data and split it to its respective test and training sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within each `DeviceDtTmDaysFromEnroll` group, we need to sort the `DeviceTm` in an ascending order. This is because the input data into the LSTM model requires each subarray to be a windowed data, which should be a time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(FILEPATH) \\\n",
    "     .sort_values(['DeviceDtTmDaysFromEnroll'], ascending=True) \\\n",
    "     .groupby(['DeviceDtTmDaysFromEnroll'], sort=False) \\\n",
    "     .apply(lambda x: x.sort_values(['DeviceTm'], ascending=True)) \\\n",
    "     .reset_index(drop=True)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint 1: Study the training data-set and answer the following \n",
    "1. How many sets of continuous training data sets do you have available\n",
    "2. The count of sample points per training data sets. Segment the data-sets into training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out unnecessary data\n",
    "\n",
    "df = df[['RecID', 'DeviceDtTmDaysFromEnroll', 'GlucoseValue']]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped = df.groupby(by='DeviceDtTmDaysFromEnroll').agg(count=('RecID', pd.Series.nunique)).reset_index()\n",
    "NO_OF_INSTANCES = len(df_grouped)\n",
    "\n",
    "print(\"There are {} continuous training data sets.\".format(NO_OF_INSTANCES))\n",
    "print(\"The count of sample points per training dataset is shown below.\")\n",
    "\n",
    "df_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataset, which is a list of list\n",
    "dataset = [[] for i in range(NO_OF_INSTANCES)]\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    group_no = row['DeviceDtTmDaysFromEnroll']\n",
    "    dataset[group_no].append(row['GlucoseValue'])\n",
    "\n",
    "print(\"There are {} datasets.\".format(len(dataset))) # a list of 11 instances\n",
    "print(\"There are {} records in day 0 dataset.\".format(len(dataset[0]))) # each containing a list of glucose values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into respective training and test sets\n",
    "train_size = int(len(dataset) * TRAIN_PCT)\n",
    "train, test = dataset[0:train_size], dataset[train_size:]\n",
    "\n",
    "print(\"There are {} train datasets.\".format(len(train))) \n",
    "print(\"There are {} test datasets.\".format(len(test))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-Processing\n",
    "We would be constructing a Many-to-one LSTM architecture, as such we require the data to be pre-processed into the necessary window size for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint 2: Display the dimensions for your pre-processed data and explain how the window size is incorporated into this structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset and labels for our training process\n",
    "# where we use sequences of 10 glucose values to generate the 11th glucose value. \n",
    "# Keras requires input to be in the shape [samples, time steps, features].\n",
    "def prepare_Xy(dataset, seq_length):\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    for i in range(0, len(dataset) - seq_length, 1):\n",
    "        seq_in = dataset[i:i + seq_length]\n",
    "        seq_out = dataset[i + seq_length]\n",
    "        X_train.append(seq_in)\n",
    "        y_train.append(seq_out)\n",
    "    seq_size = len(X_train)\n",
    "    X_train = np.reshape(X_train, (seq_size, seq_length, 1))\n",
    "    y_train = np.array(y_train)\n",
    "        \n",
    "    return X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dict = {day: prepare_Xy(train[day], FIXED_WINDOW_SIZE) for day in range(len(train))}\n",
    "test_dict = {day+len(train): prepare_Xy(test[day], FIXED_WINDOW_SIZE) for day in range(len(test))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dict, test_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras requires input to be in the shape [samples, time steps, features]. We have the following data structure as input into the model: $X = [[[x_{1}], ..., [x_{10}]], [[x_{2}], ..., [x_{11}]], ..., [[x_{n-11}], ..., [x_{n-1}]]]$, with the corresponding $y = [y_{1}, y_{2}, ..., y_{n}]$. Each day is a different continuous dataset, so the model has to be trained iteratively for each day. This is because we cannot assume that the true label of the last sequence for day 0 is the first value from day 1.\n",
    "\n",
    "In this question, the window size refers to the number of time steps. Generally, if we have a window size of 10 and GlucoseValue as the only feature, we have a single labeled sample in the following form: $X[i] = [[x_{i}], [x_{i+1}], ... ,[x_{i+9}], [x_{i+10}]]$, and $y[i] = x_{i+11}$.\n",
    "\n",
    "Extra note to self: If there are multiple features such as HeartRate, we could keep the same window size and easily add modify the data structure as such: $X[i] = [[x_{i}, u_{i}], [x_{i+1}, u_{i+1}], ... ,[x_{i+9}, u_{i+9}], [x_{i+10}, u_{i+10}]]$, and $y[i] = x_{i+11}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint 3: Explain how this many-to-one structure presented is incorporated in your data structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the above description, we require 10 timesteps to make a single prediction during training phase. We only care about the final output state at the final timestamp. Every other hidden output state in between is ignored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"many-to-one-archi.JPG\" width=\"50%\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling and Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint 4: Select the correct Loss Function and optimiser and explain the reason for your choice.\n",
    "\n",
    "I use mean squared error because my output is an unbounded continuous value. Using this error function penalizes greater difference between predicted output and the actual output more severely. This will help to reduce deviation. However, I also keep track of mean absolute error, which is a metric that is easier for users to comprehend.\n",
    "\n",
    "I use Adam due to its ability to leverage the power of adaptive learning rates methods to find individual learning rates for each parameter. Specifically, it uses the squared gradients to scale the learning rate like RMSprop and it takes advantage of momentum by using moving average of the gradient instead of gradient itself like SGD with momentum. As the algorithm combines the advantages of several other optimizers, choosing Adam therefore sounds like a reasonable approach to begin with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note on validation data:\n",
    "1. Instead of creating a separate validation set based on a full day dataset, I split each train data into train and validation during training.\n",
    "2. This allows me to keep track of the train and validation performance by each day, as the model is being trained iteratively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct our LSTM model\n",
    "\n",
    "def build_model(window_size):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(256, input_shape=(window_size, 1)))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "def train_model(model, train_dict, epochs=30, batch_size=4, callbacks=[]):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for k, (X,y) in train_dict.items():\n",
    "        print(\"=============== TRAINING ON DAY {} ===============\".format(k))\n",
    "        model_history = model.fit(X, y, validation_split=0.2, epochs=epochs, batch_size=batch_size, verbose=1, callbacks=callbacks)\n",
    "        train_losses.append(model_history.history['loss'])\n",
    "        val_losses.append(model_history.history['val_loss'])\n",
    "    \n",
    "    return flatten(train_losses), flatten(val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callbacks\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
    "mc = ModelCheckpoint('best_weights.h5', monitor='val_loss', save_best_only=True, save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train our LSTM model\n",
    "\n",
    "model = build_model(FIXED_WINDOW_SIZE)\n",
    "train_losses, val_losses = train_model(model, train_dict, callbacks=[es, mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best weights\n",
    "model.load_weights('best_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "test_input = np.array([[[53],[57],[48],[51],[52],[45],[47],[52],[54],[55]]])\n",
    "model.predict(test_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint 5: Graph and display the training loss of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_graph(train_loss_hist, val_loss_hist):\n",
    "    plt.plot(train_loss_hist, 'g', label='Training')\n",
    "    plt.plot(val_loss_hist, 'b', label='Validation')\n",
    "    plt.title('Training and Validation loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_graph(train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation\n",
    "Now that we have our trained model, we would want to validate it on the rest of the remaining data to ensure that our model is not only accurate on a single session data but for mutiple other instance. Utilise the model on the remaining data available and show that the model accuracy is acceptable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint 6: Graph the remaining instances and plot them\n",
    "Provide the true data and the validated data on the same graph, display the mean loss for each of the instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_graph(X, y):\n",
    "    _, test_mae = model.evaluate(X,y)\n",
    "    pred_y = model.predict(X)\n",
    "    \n",
    "    plt.plot(y, 'g', label='True')\n",
    "    plt.plot(pred_y, 'b', label='Predicted')\n",
    "    plt.title('Mean Absolute Loss = {}'.format(test_mae))\n",
    "    plt.xlabel('Timestep')\n",
    "    plt.ylabel('Glucose Value')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, (X,y) in test_dict.items():\n",
    "    print(\"=============== TESTING ON DAY {} ===============\".format(k))\n",
    "    evaluation_graph(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimisation\n",
    "We would now want to optimise the prediction accuracy by carrying out hyperparameter optimisation. Vary the windows size of the model and find the optimal window size that gives back the lowest loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint 7: Write a short report presenting your analysis on the optimal hyper-parameter of choice. You may include the necessary graphs or printout to explain your optimal hyper-parameter of choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter tuning should only be based on validation data, and not the train or test data. The most optimal hyperparameter is the one that produces the lowest validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_sizes = [5, 10, 15, 20, 25, 30]\n",
    "\n",
    "best_size = None\n",
    "lowest_loss = float('inf')\n",
    "window_perf_dict = dict()\n",
    "\n",
    "for size in window_sizes:\n",
    "    print(\"=============== WINDOW SIZE {} ===============\".format(size))\n",
    "    model = build_model(size)\n",
    "    train_dict = {day: prepare_Xy(train[day], size) for day in range(len(train))}\n",
    "    _, val_losses = train_model(model, train_dict)\n",
    "    model_loss = min(val_losses) # since we save the model weights by their lowest validation loss\n",
    "    window_perf_dict[size] = model_loss\n",
    "    \n",
    "    if model_loss < lowest_loss:\n",
    "        lowest_loss = model_loss\n",
    "        best_size = size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparmeter_graph(data):\n",
    "    plt.bar(range(len(data)), list(data.values()), align='center')\n",
    "    plt.xticks(range(len(data)), list(data.keys()))\n",
    "    plt.title(\"Validation loss vs window sizes\")\n",
    "    plt.xlabel('Window sizes')\n",
    "    plt.ylabel('Validation loss')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The most optimal window size is {}\".format(best_size))\n",
    "hyperparmeter_graph(window_perf_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
